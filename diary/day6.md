### 181118(Sun)
p.165 - p.222
chapter6.py : L150 - L5
##### *Remember me*

過学習が起きる要因  
1. パラメータを大量に持ち、表現力の高いモデルであること
2. 訓練データが少ないこと

荷重減衰(Weight decay)  
大きな重みを持つことに対してペナルティを課すことで、過学習を抑制

Dropout  
ニューロンをランダムに消去しながら学習することで、過学習を抑制

検証データ(Validation data)  
テストデータを使ってハイパーパラメータを調整するとテストデータに対して過学習を起こすことになる  
訓練データから20%程度を検証データとして先に分離する

ハイパーパラメータの最適化
->「良い値」が存在する範囲を徐々に絞り込んでいく  
-> ざっくり指定する
-> 10のべき乗のスケールで範囲を指定する
```
例
weigt_decay = 10 ** np.random.uniform(-8, -4)
lr = 10 ** np.random.uniform(-6, -2)
```
うまくいきそうなハイパーパラメータの範囲を観察し、値の範囲を小さくしていく。


畳み込みニューラルネットワーク(convolutional neural network)  
->CNN

全結合層(Affineレイヤ)  
データの形状が無視されてしまう

畳み込み層(Convolution レイヤ)  
形状を維持する

畳み込み演算  
->フィルター演算  
フィルター-> カーネル という

パディング : データの周囲に固定のデータ(0など)を埋める  
ストライド : フィルターを適用する窓の間隔を調節する


プーリング層(Pooling レイヤ)  
縦横の方向の空間を小さくする  
領域を一つの要素に集約する
1. 学習するパラメータがない
2. チャンネル数は変化しない
3. 微小な位置変化に対してロバスト
